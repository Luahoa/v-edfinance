# ğŸš€ AI System Optimization - REVISED PLAN (Smart Extensions)

**Date:** 2026-01-03 (Revised after Oracle Review)  
**Status:** ğŸŸ¢ PRODUCTION-READY  
**Approach:** Incremental Enhancement (80% code reuse)  
**Timeline:** 3 weeks (realistic, buffered)  

---

## ğŸ¯ EXECUTIVE SUMMARY

### Oracle's Critical Findings

| Issue | Severity | Original Plan | Revised Approach |
|-------|----------|---------------|------------------|
| **Over-Engineering** | ğŸ”´ Critical | New orchestrator service (3 files, 800+ lines) | Extend existing `ai.service.ts` (+150 lines) |
| **ORM Mismatch** | ğŸŸ  High | RAG uses Drizzle, AI uses Prisma | Add adapter layer (30 lines) |
| **Token Budget Broken** | ğŸ”´ Critical | 50k tokens = 18 queries/day | Add semantic caching â†’ 200+ queries/day |
| **Timeline Underestimate** | ğŸŸ  High | 2 weeks (50% short) | 3 weeks (realistic) |
| **Unmeasurable KPIs** | ğŸŸ  High | No baselines | Establish baselines Week 1, Day 1 |
| **Ignored Existing Assets** | ğŸŸ¡ Medium | Rebuild NudgeEngine logic | Reuse + enhance with AI variants |

**Overall Assessment:** Original plan scored **4/10** (over-complex, risky)  
**Revised Plan Target:** **8.5/10** (pragmatic, high ROI)

---

## ğŸ“Š PHASE 1: DISCOVERY (Current State - UPDATED)

### What We Already Have (Code Audit)

#### âœ… Intent Classification (ai.service.ts:403-417)
```typescript
// WORKING CODE - Don't replace, extend!
private classifyIntent(prompt: string): 'GENERAL_FAQ' | 'PERSONALIZED_ADVICE' {
  const faqKeywords = ['lÃ  gÃ¬', 'Ä‘á»‹nh nghÄ©a', 'how to', 'cÃ¡ch lÃ m'];
  const isFAQ = faqKeywords.some((k) => prompt.toLowerCase().includes(k));
  return isFAQ ? 'GENERAL_FAQ' : 'PERSONALIZED_ADVICE';
}
```

#### âœ… Context Summarization (ai.service.ts:361-375)
```typescript
// WORKING CODE - Needs improvement
private async summarizeContext(messages: any[]): Promise<string> {
  // Already summarizes old messages to save tokens
}
```

#### âœ… FAQ Caching (ai.service.ts:235-252)
```typescript
// WORKING CODE - Good foundation
private async getFAQResponse(userPrompt: string) {
  const hash = createHash('sha256').update(userPrompt).digest('hex');
  const cached = await this.cacheManager.get(`faq:${hash}`);
  // ... returns cached or generates new
}
```

#### âœ… Behavioral Nudge Engine (nudge-engine.service.ts)
```typescript
// PRODUCTION-READY - 200 lines of persona logic
async generateNudge(userId, context, data) {
  const persona = await this.analytics.getUserPersona(userId);
  
  // Lines 107-158: Sophisticated persona mapping
  if (persona === 'HUNTER') return SOCIAL_PROOF_NUDGE;
  if (persona === 'SAVER') return LOSS_AVERSION_NUDGE;
}
```

#### âœ… pgvector RAG Service (pgvector.service.ts)
```typescript
// LOCAL EMBEDDINGS - No API costs
async generateEmbedding(text: string): Promise<number[]> {
  // Xenova all-MiniLM-L6-v2 (384-dim)
  // 50ms latency, 10ms vector search
}
```

---

### ğŸš¨ Real Gaps (Based on Oracle Analysis)

#### Gap 1: RAG Not Connected to AI Mentor
**Current:** AI Mentor doesn't use RAG for factual grounding  
**Impact:** Hallucination risk (AI inventing fake financial advice)  
**Fix Complexity:** ğŸŸ¢ Low (20 lines, adapter pattern)

#### Gap 2: Token Budget Math Broken
**Current:** 50k tokens/month Ã· 2,700 tokens/query = **18 queries/day**  
**Impact:** Platform unusable for >10 active users  
**Fix Complexity:** ğŸŸ¡ Medium (semantic caching + response length limits)

#### Gap 3: No Observability Baseline
**Current:** No metrics = can't measure improvement  
**Impact:** Can't prove ROI, can't detect regressions  
**Fix Complexity:** ğŸŸ¢ Low (5 lines per method, Prometheus client)

#### Gap 4: Behavioral AI Not AI-Powered
**Current:** NudgeEngine uses rule-based templates  
**Opportunity:** AI can generate personalized variants (A/B test)  
**Fix Complexity:** ğŸŸ¢ Low (15 lines, LLM call)

---

## ğŸ¯ PHASE 2: SYNTHESIS (Smart Extensions Strategy)

### Core Principle: **"Enhance, Don't Replace"**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SMART EXTENSIONS ARCHITECTURE                     â”‚
â”‚                                                                 â”‚
â”‚  Existing AiService (ai.service.ts)                             â”‚
â”‚  â”œâ”€â”€ chat() method [ENHANCE]                                    â”‚
â”‚  â”‚   â”œâ”€â”€ classifyIntent() â†’ Add 3 new intent types âœ…          â”‚
â”‚  â”‚   â”œâ”€â”€ summarizeContext() â†’ Add entity extraction âœ…         â”‚
â”‚  â”‚   â”œâ”€â”€ NEW: getRAGContext() via adapter â­                   â”‚
â”‚  â”‚   â”œâ”€â”€ NEW: checkSemanticCache() via pgvector â­            â”‚
â”‚  â”‚   â””â”€â”€ NEW: recordMetrics() via Prometheus â­               â”‚
â”‚  â”‚                                                              â”‚
â”‚  Existing NudgeEngineService (nudge-engine.service.ts)          â”‚
â”‚  â””â”€â”€ generateNudge() [ENHANCE]                                  â”‚
â”‚      â””â”€â”€ NEW: AI variant generation (10% traffic A/B) â­       â”‚
â”‚                                                                 â”‚
â”‚  New: RAG Adapter (20 lines) â­                                 â”‚
â”‚  â””â”€â”€ Bridges Drizzle (pgvector) â†” Prisma (AiService)          â”‚
â”‚                                                                 â”‚
â”‚  New: Metrics Service (40 lines) â­                             â”‚
â”‚  â””â”€â”€ Wraps prom-client for consistency                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Total New Code:** ~200 lines (vs 2,000+ in original plan)  
**Code Reuse:** 85% (vs 20% in original plan)

---

## ğŸ”¬ PHASE 3: VERIFICATION (Measurable KPIs - FIXED)

### Establish Baselines First (Week 1, Day 1)

```bash
# Run this BEFORE making changes
curl http://localhost:3001/ai/metrics/baseline > baseline-2026-01-03.json

# Expected output:
{
  "avgLatency": "3.2s",          # â† Baseline for improvement
  "avgTokensPerRequest": 2800,   # â† Baseline for efficiency
  "dailyBudgetUsage": "85%",     # â† Baseline for cost
  "cacheHitRate": 0.12,          # â† Only 12% (FAQ only)
  "hallucinationSamples": []     # â† Manual evaluation needed
}
```

### Target Metrics (Week 3 - Measurable vs Baseline)

| Metric | Baseline | Target | How to Measure |
|--------|----------|--------|----------------|
| **Response Latency (P95)** | 3.2s | <2s (-38%) | Prometheus histogram |
| **Tokens per Request** | 2,800 | <1,500 (-46%) | Counter in chat() |
| **Daily Budget Usage** | 85% | <50% | (Used tokens / 50k) Ã— 100 |
| **Cache Hit Rate** | 12% | >60% (+400%) | (Hits / Total queries) Ã— 100 |
| **User Satisfaction** | Unknown | 4.2/5 | Post-chat thumbs up/down |
| **RAG Grounding Rate** | 0% | >80% | % responses citing sources |

### A/B Testing Framework (Week 3)

```typescript
// Add to ai.service.ts
async chat(...) {
  const variant = this.getABVariant(userId); // 50/50 split
  
  if (variant === 'CONTROL') {
    return this.chatOriginal(...); // No changes
  } else {
    return this.chatSmartExtensions(...); // With RAG + caching
  }
}
```

**Sample Size:** 100 users per variant (200 total)  
**Duration:** 1 week  
**Success Criteria:** +15% satisfaction, -30% token cost

---

## ğŸ› ï¸ PHASE 4: DECOMPOSITION (Implementation Tasks - REVISED)

### Week 1: Foundation + Baselines (5 days)

#### ğŸ”· EPIC 1: Observability First (Day 1-2)

**Why First?** Can't improve what you can't measure.

##### Task 1.1: Metrics Baseline Endpoint (4 hours)
```typescript
// apps/api/src/ai/metrics.service.ts (NEW FILE - 40 lines)

import { Injectable } from '@nestjs/common';
import { Counter, Histogram } from 'prom-client';

@Injectable()
export class AiMetricsService {
  private latencyHistogram = new Histogram({
    name: 'ai_response_latency_seconds',
    help: 'AI response time',
    buckets: [0.5, 1, 2, 3, 5, 10],
  });
  
  private tokenCounter = new Counter({
    name: 'ai_tokens_used_total',
    help: 'Total tokens consumed',
    labelNames: ['intent'],
  });
  
  recordLatency(durationMs: number) {
    this.latencyHistogram.observe(durationMs / 1000);
  }
  
  recordTokens(count: number, intent: string) {
    this.tokenCounter.inc({ intent }, count);
  }
}
```

**Deliverable:** `/api/metrics` endpoint returns Prometheus metrics  
**Test:** `curl http://localhost:3001/metrics | grep ai_`

**Beads Task:**
```bash
beads.exe create "Task 1.1: AI Metrics Service + Baseline Endpoint" \
  --type task \
  --priority 1 \
  --estimate "4h" \
  --tags "observability,week1"
```

##### Task 1.2: Integrate Metrics into AiService (2 hours)
```typescript
// apps/api/src/ai/ai.service.ts (ENHANCE EXISTING)

export class AiService {
  constructor(
    // ... existing
    private metricsService: AiMetricsService, // â† ADD
  ) {}
  
  async chat(threadId: string, userId: string, userPrompt: string) {
    const startTime = Date.now(); // â† ADD
    
    try {
      // ... existing logic (lines 160-230)
      
      const tokensUsed = this.estimateTokens(prompt + response);
      this.metricsService.recordTokens(tokensUsed, intent); // â† ADD
      
      return response;
    } finally {
      this.metricsService.recordLatency(Date.now() - startTime); // â† ADD
    }
  }
}
```

**Deliverable:** All AI calls tracked in Prometheus  
**Test:** Run 10 queries, verify `ai_response_latency_seconds_count{} 10`

**Beads Task:**
```bash
beads.exe create "Task 1.2: Integrate Metrics into AiService.chat()" \
  --type task \
  --priority 1 \
  --deps "ved-xxx (Task 1.1)" \
  --estimate "2h"
```

##### Task 1.3: Establish Baseline Report (1 hour)
```bash
# Create baseline snapshot
curl http://localhost:3001/metrics > baselines/ai-metrics-2026-01-03.txt

# Run 50 test queries to warm up metrics
for i in {1..50}; do
  curl -X POST http://localhost:3001/ai/threads/test-thread/chat \
    -H "Authorization: Bearer $TOKEN" \
    -d '{"prompt": "LÃ£i suáº¥t kÃ©p lÃ  gÃ¬?"}'
done

# Extract key metrics
cat baselines/ai-metrics-2026-01-03.txt | grep 'ai_response_latency_seconds{quantile="0.95"}'
# â†’ ai_response_latency_seconds{quantile="0.95"} 3.2
```

**Deliverable:** `baselines/ai-metrics-2026-01-03.txt` committed to repo  
**Beads Task:** (Manual QA task, no code)

---

#### ğŸ”· EPIC 2: RAG Integration (Day 3-4)

##### Task 2.1: RAG Adapter Service (3 hours)
```typescript
// apps/api/src/ai/rag-adapter.service.ts (NEW FILE - 30 lines)

import { Injectable } from '@nestjs/common';
import { PgvectorService } from '../database/pgvector.service';

export interface RAGContext {
  sources: Array<{
    title: string;
    content: string;
    similarity: number;
  }>;
  combinedText: string;
}

@Injectable()
export class RagAdapterService {
  constructor(private pgvector: PgvectorService) {}
  
  async getRelevantContext(query: string): Promise<RAGContext> {
    // 1. Generate embedding for query
    const embedding = await this.pgvector.generateEmbedding(query);
    
    // 2. Find similar documents (uses Drizzle internally)
    const similar = await this.pgvector.findSimilarOptimizations(embedding, {
      threshold: 0.75,
      limit: 3,
    });
    
    // 3. Format for Gemini prompt
    return {
      sources: similar.map(s => ({
        title: s.queryText || 'Untitled',
        content: s.recommendation || '',
        similarity: s.similarity,
      })),
      combinedText: similar
        .map(s => `[Source: ${s.queryText}]\n${s.recommendation}`)
        .join('\n\n'),
    };
  }
}
```

**Deliverable:** RAG adapter bridges Drizzle/Prisma ORM gap  
**Test:** Unit test with mock pgvector service

**Beads Task:**
```bash
beads.exe create "Task 2.1: RAG Adapter Service (Drizzle â†” Prisma Bridge)" \
  --type task \
  --priority 1 \
  --estimate "3h" \
  --tags "rag,week1"
```

##### Task 2.2: Connect RAG to AiService.chat() (4 hours)
```typescript
// apps/api/src/ai/ai.service.ts (ENHANCE EXISTING chat method)

async chat(threadId: string, userId: string, userPrompt: string) {
  const startTime = Date.now();
  const intent = this.classifyIntent(userPrompt);
  
  // âœ… NEW: Retrieve RAG context for non-FAQ queries
  let ragContext = '';
  if (intent !== 'GENERAL_FAQ' && this.shouldUseRAG(userPrompt)) {
    const rag = await this.ragAdapter.getRelevantContext(userPrompt);
    
    if (rag.sources.length > 0) {
      ragContext = `
RELEVANT KNOWLEDGE (cite these sources in your response):
${rag.combinedText}
`;
      
      // Track RAG usage
      this.metricsService.recordEvent('rag_retrieval', {
        sources_found: rag.sources.length,
        avg_similarity: rag.sources.reduce((a, s) => a + s.similarity, 0) / rag.sources.length,
      });
    }
  }
  
  // Build enhanced system instruction
  const systemInstruction = `
${this.getBaseSystemPrompt(userId)}

${ragContext}

USER QUERY: ${userPrompt}

INSTRUCTIONS:
- If you used the RELEVANT KNOWLEDGE above, cite the source in your response
- If no relevant knowledge, say "I need more information to answer this accurately"
- Keep response under 200 words
`;
  
  // ... rest of existing logic
}

private shouldUseRAG(prompt: string): boolean {
  // Use RAG for:
  // - Financial questions (investment, portfolio, risk)
  // - Course recommendations
  // Skip RAG for:
  // - Greetings, small talk
  // - Already cached responses
  
  const ragKeywords = [
    'Ä‘áº§u tÆ°', 'investment', 'portfolio', 
    'rá»§i ro', 'risk', 'khÃ³a há»c', 'course'
  ];
  return ragKeywords.some(k => prompt.toLowerCase().includes(k));
}
```

**Deliverable:** AI Mentor cites sources from RAG (80%+ grounding rate)  
**Test:** Ask "LÃ£i suáº¥t kÃ©p lÃ  gÃ¬?" â†’ Response cites course content

**Beads Task:**
```bash
beads.exe create "Task 2.2: Connect RAG to AiService.chat() - Grounding" \
  --type task \
  --priority 1 \
  --deps "ved-xxx (Task 2.1)" \
  --estimate "4h"
```

##### Task 2.3: Seed Financial Knowledge Base (6 hours)
```typescript
// apps/api/src/scripts/seed-rag-financial.ts (NEW SCRIPT)

import { PrismaClient } from '@prisma/client';
import { PgvectorService } from '../database/pgvector.service';
import * as fs from 'fs';
import * as path from 'path';

async function seedFinancialDocs() {
  const prisma = new PrismaClient();
  const pgvector = new PgvectorService(/* ... */);
  
  // 1. Load financial education docs (Markdown)
  const docsDir = path.join(__dirname, '../../docs/financial-education');
  const files = fs.readdirSync(docsDir).filter(f => f.endsWith('.md'));
  
  for (const file of files) {
    const content = fs.readFileSync(path.join(docsDir, file), 'utf-8');
    
    // Extract title from # heading
    const titleMatch = content.match(/^#\s+(.+)$/m);
    const title = titleMatch ? titleMatch[1] : file.replace('.md', '');
    
    // Generate embedding
    const embedding = await pgvector.generateEmbedding(content);
    
    // Store in knowledge base
    await prisma.optimizationLog.create({
      data: {
        queryText: title,
        recommendation: content,
        performanceGain: 0, // Not applicable for docs
        embedding: embedding, // pgvector will handle this
        metadata: {
          type: 'financial_doc',
          source: file,
        },
      },
    });
    
    console.log(`âœ… Seeded: ${title}`);
  }
  
  console.log(`\nğŸ‰ Seeded ${files.length} financial documents`);
}

seedFinancialDocs();
```

**Content to Create (Manual):**
```
docs/financial-education/
â”œâ”€â”€ compound-interest.md         # LÃ£i suáº¥t kÃ©p
â”œâ”€â”€ risk-diversification.md      # Äa dáº¡ng hÃ³a rá»§i ro
â”œâ”€â”€ portfolio-allocation.md      # PhÃ¢n bá»• danh má»¥c
â”œâ”€â”€ tax-optimization.md          # Tá»‘i Æ°u thuáº¿
â””â”€â”€ emergency-fund.md            # Quá»¹ kháº©n cáº¥p
```

**Deliverable:** 5+ financial docs in RAG knowledge base  
**Test:** Query "What is compound interest?" â†’ RAG returns compound-interest.md

**Beads Task:**
```bash
beads.exe create "Task 2.3: Seed RAG with Financial Education Docs" \
  --type task \
  --priority 2 \
  --deps "ved-xxx (Task 2.2)" \
  --estimate "6h" \
  --tags "content,week1"
```

---

#### ğŸ”· EPIC 3: Token Optimization (Day 5)

##### Task 3.1: Semantic Response Caching (5 hours)
```typescript
// apps/api/src/ai/ai.service.ts (ENHANCE EXISTING)

async chat(threadId: string, userId: string, userPrompt: string) {
  const startTime = Date.now();
  
  // âœ… NEW: Check semantic cache (not just exact FAQ)
  const cachedResponse = await this.getSemanticCachedResponse(userPrompt);
  if (cachedResponse) {
    this.metricsService.recordEvent('cache_hit', { type: 'semantic' });
    return cachedResponse;
  }
  
  // ... rest of logic (RAG, LLM call, etc.)
  
  // After generating response, cache it
  await this.cacheSemanticResponse(userPrompt, response);
  
  return response;
}

private async getSemanticCachedResponse(query: string): Promise<string | null> {
  // 1. Generate embedding for query
  const embedding = await this.ragAdapter.pgvector.generateEmbedding(query);
  
  // 2. Find similar cached responses (high threshold = exact match)
  const similar = await this.ragAdapter.pgvector.findSimilarOptimizations(embedding, {
    threshold: 0.92, // Very high = near-identical queries
    limit: 1,
  });
  
  if (similar.length > 0) {
    // Found a cached response for similar query
    return similar[0].recommendation;
  }
  
  return null;
}

private async cacheSemanticResponse(query: string, response: string) {
  const embedding = await this.ragAdapter.pgvector.generateEmbedding(query);
  
  // Store in optimization_logs table (reuse existing schema)
  await this.prisma.optimizationLog.create({
    data: {
      queryText: query,
      recommendation: response,
      embedding: embedding,
      metadata: {
        type: 'semantic_cache',
        cached_at: new Date().toISOString(),
      },
    },
  });
}
```

**Impact:** Cache hit rate 12% â†’ 60%+ (5x improvement)  
**Token Savings:** 60% of queries skip LLM = 60% Ã— 2,700 tokens = 1,620 tokens saved per cached query

**Beads Task:**
```bash
beads.exe create "Task 3.1: Semantic Response Caching via pgvector" \
  --type task \
  --priority 1 \
  --estimate "5h" \
  --tags "optimization,week1"
```

##### Task 3.2: Response Length Limits (2 hours)
```typescript
// apps/api/src/ai/ai.service.ts (ENHANCE EXISTING)

async chat(...) {
  // ... existing logic
  
  const response = await this.model.generateContent({
    contents: [{ parts: [{ text: systemInstruction }] }],
    generationConfig: {
      maxOutputTokens: 500,  // âœ… NEW: Hard limit (was unlimited)
      temperature: 0.7,
      topP: 0.9,
    },
  });
  
  // ... rest
}
```

**Impact:** Average response length 800 tokens â†’ 400 tokens (50% reduction)  
**Deliverable:** Update Gemini API call config

**Beads Task:**
```bash
beads.exe create "Task 3.2: Add Response Length Limits (maxOutputTokens)" \
  --type task \
  --priority 2 \
  --estimate "2h"
```

---

### Week 2: Enhanced Intelligence (5 days)

#### ğŸ”· EPIC 4: Multi-Intent Classification (Day 6-7)

##### Task 4.1: Extend Intent Classifier (4 hours)
```typescript
// apps/api/src/ai/ai.service.ts (ENHANCE EXISTING METHOD)

export enum QueryIntent {
  GENERAL_FAQ = 'GENERAL_FAQ',               // Existing
  PERSONALIZED_ADVICE = 'PERSONALIZED_ADVICE', // Existing
  FINANCIAL_ANALYSIS = 'FINANCIAL_ANALYSIS',   // âœ… NEW
  LEARNING_SUPPORT = 'LEARNING_SUPPORT',       // âœ… NEW
  BEHAVIORAL_NUDGE = 'BEHAVIORAL_NUDGE',       // âœ… NEW
}

private classifyIntent(prompt: string): QueryIntent[] {
  // Support multiple intents (complex queries)
  const intents: QueryIntent[] = [];
  
  const keywords = {
    FINANCIAL_ANALYSIS: ['portfolio', 'danh má»¥c', 'risk', 'rá»§i ro', 'invest', 'Ä‘áº§u tÆ°'],
    LEARNING_SUPPORT: ['course', 'khÃ³a há»c', 'lesson', 'bÃ i há»c', 'recommend', 'gá»£i Ã½'],
    BEHAVIORAL_NUDGE: ['streak', 'thÃ³i quen', 'motivation', 'Ä‘á»™ng lá»±c'],
    GENERAL_FAQ: ['lÃ  gÃ¬', 'what is', 'how to', 'cÃ¡ch lÃ m'],
  };
  
  for (const [intent, words] of Object.entries(keywords)) {
    if (words.some(w => prompt.toLowerCase().includes(w))) {
      intents.push(intent as QueryIntent);
    }
  }
  
  // Default: PERSONALIZED_ADVICE
  return intents.length > 0 ? intents : [QueryIntent.PERSONALIZED_ADVICE];
}
```

**Deliverable:** Classifier supports 5 intent types (was 2)  
**Test:** "Recommend a course about portfolio risk" â†’ `['LEARNING_SUPPORT', 'FINANCIAL_ANALYSIS']`

**Beads Task:**
```bash
beads.exe create "Task 4.1: Extend Intent Classifier (2 â†’ 5 types)" \
  --type task \
  --priority 1 \
  --estimate "4h" \
  --tags "intelligence,week2"
```

##### Task 4.2: Intent-Specific RAG Filtering (3 hours)
```typescript
// apps/api/src/ai/rag-adapter.service.ts (ENHANCE EXISTING)

async getRelevantContext(query: string, intents: QueryIntent[]): Promise<RAGContext> {
  const embedding = await this.pgvector.generateEmbedding(query);
  
  // âœ… Filter by intent type
  const metadataFilter = this.buildMetadataFilter(intents);
  
  const similar = await this.pgvector.findSimilarOptimizations(embedding, {
    threshold: 0.75,
    limit: 3,
    // NEW: Filter by document type based on intent
    where: metadataFilter,
  });
  
  return { /* ... */ };
}

private buildMetadataFilter(intents: QueryIntent[]) {
  // Map intents to document types
  const typeMap = {
    [QueryIntent.FINANCIAL_ANALYSIS]: ['financial_doc', 'portfolio_example'],
    [QueryIntent.LEARNING_SUPPORT]: ['course_lesson', 'quiz_explanation'],
    [QueryIntent.BEHAVIORAL_NUDGE]: ['nudge_template', 'behavioral_principle'],
  };
  
  const allowedTypes = intents.flatMap(i => typeMap[i] || []);
  
  return allowedTypes.length > 0 
    ? { metadata: { type: { in: allowedTypes } } }
    : {};
}
```

**Deliverable:** RAG retrieves intent-relevant docs only  
**Test:** Financial query â†’ returns financial_doc, not course_lesson

**Beads Task:**
```bash
beads.exe create "Task 4.2: Intent-Specific RAG Filtering" \
  --type task \
  --priority 2 \
  --deps "ved-xxx (Task 4.1)" \
  --estimate "3h"
```

---

#### ğŸ”· EPIC 5: Behavioral AI Fusion (Day 8-9)

##### Task 5.1: AI Nudge Variant Generator (6 hours)
```typescript
// apps/api/src/modules/nudge/nudge-engine.service.ts (ENHANCE EXISTING)

export class NudgeEngineService {
  constructor(
    private prisma: PrismaService,
    private analytics: AnalyticsService,
    private aiService: AiService, // âœ… NEW: Inject AiService
  ) {}
  
  async generateNudge(userId: string, context: string, data: any) {
    // Get base rule-based nudge (existing logic)
    const baseNudge = await this.getRuleBasedNudge(userId, context, data);
    
    if (!baseNudge) return null;
    
    // âœ… NEW: A/B test AI variants (10% of traffic)
    const shouldUseAI = Math.random() < 0.1;
    
    if (shouldUseAI) {
      const aiVariant = await this.generateAIVariant(baseNudge, userId);
      
      // Track which variant was used
      await this.trackNudgeVariant(userId, {
        base: baseNudge,
        ai: aiVariant,
        variant: 'AI',
      });
      
      return aiVariant;
    }
    
    await this.trackNudgeVariant(userId, {
      base: baseNudge,
      variant: 'RULE_BASED',
    });
    
    return baseNudge;
  }
  
  private async generateAIVariant(baseNudge: any, userId: string) {
    const user = await this.prisma.user.findUnique({ where: { id: userId } });
    
    const prompt = `
Generate a personalized variant of this behavioral nudge:

BASE MESSAGE: ${JSON.stringify(baseNudge.message)}
NUDGE TYPE: ${baseNudge.type}
USER CONTEXT:
- Knowledge Level: ${user.knowledgeLevel}
- Preferred Language: ${user.locale}
- Streak: ${user.streak} days

REQUIREMENTS:
1. Keep the same nudge type (${baseNudge.type})
2. Make it more personal and engaging
3. Use metaphors related to Vietnamese culture if locale=vi
4. Keep under 50 words
5. Output in JSON format: {"vi": "...", "en": "...", "zh": "..."}
`;
    
    const response = await this.aiService.generateQuickResponse(prompt);
    
    return {
      type: baseNudge.type,
      message: JSON.parse(response),
      priority: baseNudge.priority,
      metadata: {
        generatedBy: 'AI',
        baseTemplate: baseNudge.message.vi,
      },
    };
  }
  
  private async trackNudgeVariant(userId: string, data: any) {
    await this.prisma.behaviorLog.create({
      data: {
        userId,
        eventType: 'NUDGE_VARIANT_TEST',
        actionCategory: 'ENGAGEMENT',
        payload: data,
      },
    });
  }
}
```

**Deliverable:** 10% of nudges use AI-generated variants (A/B test)  
**Test:** Trigger STREAK_WARNING nudge â†’ 1/10 times gets AI variant

**Beads Task:**
```bash
beads.exe create "Task 5.1: AI Nudge Variant Generator (A/B Test)" \
  --type task \
  --priority 1 \
  --estimate "6h" \
  --tags "behavioral,week2"
```

##### Task 5.2: Proactive AI Trigger Service (4 hours)
```typescript
// apps/api/src/ai/proactive-triggers.service.ts (NEW FILE - 60 lines)

import { Injectable } from '@nestjs/common';
import { Cron } from '@nestjs/schedule';
import { PrismaService } from '../prisma/prisma.service';
import { NudgeEngineService } from '../modules/nudge/nudge-engine.service';

@Injectable()
export class ProactiveTriggersService {
  constructor(
    private prisma: PrismaService,
    private nudgeEngine: NudgeEngineService,
  ) {}
  
  @Cron('0 */4 * * *') // Every 4 hours
  async checkStreaksAtRisk() {
    const now = new Date();
    const yesterday = new Date(now.getTime() - 24 * 60 * 60 * 1000);
    
    // Find users who haven't logged in today
    const usersAtRisk = await this.prisma.user.findMany({
      where: {
        lastLoginAt: { lt: yesterday },
        streak: { gte: 3 }, // Only care about 3+ day streaks
      },
    });
    
    for (const user of usersAtRisk) {
      const hoursLeft = 24 - (now.getTime() - user.lastLoginAt.getTime()) / (1000 * 60 * 60);
      
      if (hoursLeft < 6) { // Urgent: <6 hours left
        const nudge = await this.nudgeEngine.generateNudge(
          user.id,
          'STREAK_WARNING',
          { streak: user.streak, hoursLeft: Math.floor(hoursLeft) },
        );
        
        if (nudge) {
          // Send push notification (integrate with existing NotificationService)
          await this.sendPushNotification(user.id, nudge.message[user.locale]);
        }
      }
    }
  }
  
  @Cron('0 9 * * *') // Daily at 9 AM
  async checkUnfinishedCourses() {
    // Find users with >80% course completion
    const almostDone = await this.prisma.courseProgress.findMany({
      where: {
        progress: { gte: 80, lt: 100 },
        updatedAt: { lt: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000) }, // Stale 7+ days
      },
      include: { user: true, course: true },
    });
    
    for (const cp of almostDone) {
      const nudge = await this.nudgeEngine.generateNudge(
        cp.user.id,
        'COURSE_COMPLETION',
        { courseTitle: cp.course.title, progress: cp.progress },
      );
      
      if (nudge) {
        await this.sendPushNotification(cp.user.id, nudge.message[cp.user.locale]);
      }
    }
  }
  
  private async sendPushNotification(userId: string, message: string) {
    // Implement via existing NotificationService (Firebase, OneSignal, etc.)
    console.log(`ğŸ“² Sending to ${userId}: ${message}`);
  }
}
```

**Deliverable:** Cron jobs trigger proactive AI nudges (Hooked Loop: External Trigger)  
**Test:** Set user.lastLoginAt to 20 hours ago â†’ Nudge sent at next cron run

**Beads Task:**
```bash
beads.exe create "Task 5.2: Proactive AI Trigger Service (Cron Jobs)" \
  --type task \
  --priority 2 \
  --estimate "4h" \
  --tags "behavioral,hooked-loop,week2"
```

---

### Week 3: Validation + Polish (5 days)

#### ğŸ”· EPIC 6: Testing + Documentation (Day 10-12)

##### Task 6.1: Unit Tests (8 hours)
```typescript
// apps/api/src/ai/rag-adapter.service.spec.ts

describe('RagAdapterService', () => {
  it('should retrieve relevant context for financial query', async () => {
    const context = await service.getRelevantContext(
      'What is portfolio diversification?',
      [QueryIntent.FINANCIAL_ANALYSIS]
    );
    
    expect(context.sources.length).toBeGreaterThan(0);
    expect(context.sources[0].title).toContain('diversification');
  });
  
  it('should filter by intent type', async () => {
    const context = await service.getRelevantContext(
      'Recommend a course',
      [QueryIntent.LEARNING_SUPPORT]
    );
    
    // Should NOT return financial docs
    expect(context.sources.every(s => s.metadata?.type !== 'financial_doc')).toBe(true);
  });
});

// apps/api/src/ai/ai.service.spec.ts

describe('AiService - Semantic Caching', () => {
  it('should return cached response for similar query', async () => {
    // First query (cache miss)
    await service.chat('thread1', 'user1', 'What is compound interest?');
    
    // Similar query (cache hit)
    const response = await service.chat('thread1', 'user1', 'Explain compound interest');
    
    expect(response).toBeDefined();
    // Verify cache hit metric incremented
  });
});
```

**Target:** 60% coverage (focus on new code: RAG adapter, semantic cache)  
**Deliverable:** Jest test suite passes

**Beads Task:**
```bash
beads.exe create "Task 6.1: Unit Tests (60% Coverage for New Code)" \
  --type task \
  --priority 1 \
  --estimate "8h" \
  --tags "testing,week3"
```

##### Task 6.2: Integration Tests (6 hours)
```typescript
// apps/api/test/ai-smart-extensions.e2e.spec.ts

describe('AI Smart Extensions E2E', () => {
  it('should ground response in RAG knowledge', async () => {
    const response = await request(app.getHttpServer())
      .post('/ai/threads/test-thread/chat')
      .set('Authorization', `Bearer ${token}`)
      .send({ prompt: 'What is compound interest?' });
    
    expect(response.body).toMatch(/\[Source:/); // Cites source
  });
  
  it('should use semantic cache for similar queries', async () => {
    // Query 1
    await request(app.getHttpServer())
      .post('/ai/threads/test-thread/chat')
      .send({ prompt: 'Portfolio risk?' });
    
    // Query 2 (similar)
    const start = Date.now();
    await request(app.getHttpServer())
      .post('/ai/threads/test-thread/chat')
      .send({ prompt: 'Risk in my portfolio?' });
    const duration = Date.now() - start;
    
    expect(duration).toBeLessThan(500); // Fast = cache hit
  });
});
```

**Deliverable:** E2E tests verify RAG grounding + caching

**Beads Task:**
```bash
beads.exe create "Task 6.2: Integration Tests (RAG + Caching E2E)" \
  --type task \
  --priority 1 \
  --estimate "6h"
```

##### Task 6.3: Manual Evaluation (10 Test Queries) (2 hours)
```markdown
# Manual Evaluation Checklist

Test each query and verify:
âœ… Response cites RAG sources (when applicable)
âœ… Response is factually correct (verified against docs)
âœ… Response is localized (correct language)
âœ… Latency <2s (P95)
âœ… Tokens used <1,500 (check metrics)

| Query | Expected Intent | Cited Sources? | Factually Correct? | Latency | Tokens |
|-------|----------------|----------------|-------------------|---------|--------|
| "LÃ£i suáº¥t kÃ©p lÃ  gÃ¬?" | GENERAL_FAQ | âœ… Yes | âœ… Yes | 1.2s | 800 |
| "Recommend course about stocks" | LEARNING_SUPPORT | âœ… Yes | âœ… Yes | 1.8s | 1,200 |
| "What's my portfolio risk?" | FINANCIAL_ANALYSIS | âš ï¸ No data | N/A | 1.5s | 900 |
| ... (7 more) | | | | | |
```

**Deliverable:** `docs/AI_MANUAL_EVALUATION_2026-01-10.md` with results

**Beads Task:**
```bash
beads.exe create "Task 6.3: Manual Evaluation (10 Test Queries)" \
  --type task \
  --priority 2 \
  --estimate "2h"
```

---

#### ğŸ”· EPIC 7: A/B Testing + Launch (Day 13-15)

##### Task 7.1: A/B Test Setup (4 hours)
```typescript
// apps/api/src/ai/ab-testing.service.ts (NEW FILE - 50 lines)

import { Injectable } from '@nestjs/common';
import { PrismaService } from '../prisma/prisma.service';

export enum ABVariant {
  CONTROL = 'CONTROL',        // Original system (no changes)
  TREATMENT = 'TREATMENT',    // Smart Extensions (RAG + caching)
}

@Injectable()
export class ABTestingService {
  constructor(private prisma: PrismaService) {}
  
  async getVariant(userId: string): Promise<ABVariant> {
    // Check if user already assigned
    let assignment = await this.prisma.aBTestAssignment.findUnique({
      where: { userId },
    });
    
    if (!assignment) {
      // New user: 50/50 split
      const variant = Math.random() < 0.5 ? ABVariant.CONTROL : ABVariant.TREATMENT;
      
      assignment = await this.prisma.aBTestAssignment.create({
        data: {
          userId,
          variant,
          testName: 'AI_SMART_EXTENSIONS_2026_01',
        },
      });
    }
    
    return assignment.variant as ABVariant;
  }
  
  async recordFeedback(userId: string, threadId: string, rating: number) {
    await this.prisma.behaviorLog.create({
      data: {
        userId,
        eventType: 'AI_FEEDBACK',
        actionCategory: 'ENGAGEMENT',
        payload: {
          threadId,
          rating, // 1-5 stars (or thumbs up/down)
          variant: await this.getVariant(userId),
        },
      },
    });
  }
}
```

**Migration Needed:**
```sql
-- prisma/migrations/xxx_ab_test_assignments.sql
CREATE TABLE ab_test_assignments (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id),
  variant VARCHAR(20) NOT NULL,
  test_name VARCHAR(100) NOT NULL,
  created_at TIMESTAMP DEFAULT NOW(),
  UNIQUE(user_id, test_name)
);
```

**Deliverable:** A/B test framework ready, 50/50 user split

**Beads Task:**
```bash
beads.exe create "Task 7.1: A/B Test Framework (Control vs Treatment)" \
  --type task \
  --priority 1 \
  --estimate "4h" \
  --tags "ab-testing,week3"
```

##### Task 7.2: Grafana Dashboard (4 hours)
```json
// monitoring/grafana/dashboards/ai-performance.json

{
  "title": "AI System Performance",
  "panels": [
    {
      "title": "Response Latency (P50/P95)",
      "targets": [
        {
          "expr": "histogram_quantile(0.5, ai_response_latency_seconds_bucket)",
          "legendFormat": "P50"
        },
        {
          "expr": "histogram_quantile(0.95, ai_response_latency_seconds_bucket)",
          "legendFormat": "P95"
        }
      ]
    },
    {
      "title": "Token Usage (Last Hour)",
      "targets": [
        {
          "expr": "rate(ai_tokens_used_total[1h])"
        }
      ]
    },
    {
      "title": "Cache Hit Rate",
      "targets": [
        {
          "expr": "rate(ai_cache_hits[5m]) / rate(ai_total_queries[5m])"
        }
      ]
    },
    {
      "title": "RAG Grounding Rate",
      "targets": [
        {
          "expr": "rate(rag_retrieval_success[5m]) / rate(ai_total_queries[5m])"
        }
      ]
    }
  ]
}
```

**Deliverable:** Grafana dashboard visualizes KPIs in real-time

**Beads Task:**
```bash
beads.exe create "Task 7.2: Grafana Dashboard (AI Performance KPIs)" \
  --type task \
  --priority 2 \
  --estimate "4h"
```

##### Task 7.3: Documentation Update (3 hours)
```markdown
# AI System Documentation Update

## Files to Update:

1. **AGENTS.md** - Add new section:
   ```markdown
   ### AI Smart Extensions (Deployed: 2026-01-10)
   - RAG-grounded responses (80%+ citation rate)
   - Semantic caching (60% hit rate)
   - Multi-intent classification (5 types)
   - AI-powered nudge variants (10% A/B test)
   ```

2. **README.md** - Update AI features section

3. **docs/AI_ARCHITECTURE.md** (NEW) - Document:
   - Smart Extensions architecture diagram
   - RAG integration flow
   - Semantic caching mechanism
   - A/B testing methodology

4. **API Documentation** - Add new endpoints:
   - `POST /ai/feedback` (thumbs up/down)
   - `GET /ai/metrics/baseline` (for monitoring)
```

**Beads Task:**
```bash
beads.exe create "Task 7.3: Update Documentation (AGENTS.md + API Docs)" \
  --type task \
  --priority 2 \
  --estimate "3h"
```

---

## âœ… PHASE 5: VALIDATION (Testing Strategy - ENHANCED)

### Week 1: Continuous Testing
- **Day 1-2:** Metrics baseline established â†’ automated regression tests
- **Day 3-5:** Unit tests written alongside implementation (TDD approach)

### Week 2: Integration Testing
- **Day 8-9:** E2E tests for RAG + caching
- **Day 10:** Load testing (50 concurrent users, 100 queries)

### Week 3: A/B Testing
- **Day 13-15:** Monitor A/B test results
  - **Sample Size:** 200 users (100 per variant)
  - **Duration:** 1 week
  - **Success Criteria:**
    - +15% user satisfaction (thumbs up rate)
    - -30% token cost per query
    - +10% engagement (queries per user)

### Manual Evaluation Rubric

| Criterion | Weight | Scoring |
|-----------|--------|---------|
| **Factual Accuracy** | 40% | 0-10 scale (verified against docs) |
| **RAG Citation** | 20% | Binary (cited sources? Y/N) |
| **Localization** | 15% | Correct language + cultural context |
| **Latency** | 15% | <2s = 10, 2-3s = 5, >3s = 0 |
| **Token Efficiency** | 10% | <1500 = 10, 1500-2000 = 5, >2000 = 0 |

**Pass Threshold:** Average score â‰¥8/10 across 10 test queries

---

## ğŸ“ˆ PHASE 6: TRACK PLANNING (Beads Tasks - REVISED)

### Sprint Structure (3 Weeks, Parallel Execution)

```
Week 1: Foundation
â”œâ”€â”€ Epic 1: Observability (parallel) â”€â”€â”€â”€â”€â”
â”‚   â”œâ”€â”€ Task 1.1: Metrics Service (4h)    â”‚
â”‚   â”œâ”€â”€ Task 1.2: Integrate Metrics (2h)  â”‚
â”‚   â””â”€â”€ Task 1.3: Baseline Report (1h)    â”‚
â”œâ”€â”€ Epic 2: RAG Integration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â†’ All run in parallel
â”‚   â”œâ”€â”€ Task 2.1: RAG Adapter (3h)        â”‚
â”‚   â”œâ”€â”€ Task 2.2: Connect to AI (4h)      â”‚
â”‚   â””â”€â”€ Task 2.3: Seed Docs (6h)          â”‚
â””â”€â”€ Epic 3: Token Optimization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”œâ”€â”€ Task 3.1: Semantic Caching (5h)
    â””â”€â”€ Task 3.2: Response Limits (2h)

Week 2: Intelligence
â”œâ”€â”€ Epic 4: Multi-Intent (parallel) â”€â”€â”€â”€â”€â”€â”
â”‚   â”œâ”€â”€ Task 4.1: Extend Classifier (4h)  â”‚
â”‚   â””â”€â”€ Task 4.2: Intent Filtering (3h)   â”‚
â””â”€â”€ Epic 5: Behavioral AI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â†’ Parallel
    â”œâ”€â”€ Task 5.1: AI Nudge Variants (6h)  â”‚
    â””â”€â”€ Task 5.2: Proactive Triggers (4h) â”‚

Week 3: Validation
â”œâ”€â”€ Epic 6: Testing (parallel) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â”œâ”€â”€ Task 6.1: Unit Tests (8h)         â”‚
â”‚   â”œâ”€â”€ Task 6.2: Integration Tests (6h)  â”‚
â”‚   â””â”€â”€ Task 6.3: Manual Eval (2h)        â”‚
â””â”€â”€ Epic 7: Launch Prep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”œâ”€â”€ Task 7.1: A/B Test Setup (4h)
    â”œâ”€â”€ Task 7.2: Grafana Dashboard (4h)
    â””â”€â”€ Task 7.3: Documentation (3h)
```

**Total Effort:** 71 hours (vs 100+ in original plan)  
**Critical Path:** Week 1 Epic 2 (RAG) â†’ Week 2 Epic 4 â†’ Week 3 Epic 6  
**Buffer:** 4 days (Week 3 has only 60% utilization for unexpected issues)

---

### Beads Task Creation Script

```bash
# Week 1
beads.exe create "Epic 1: Observability First" --type epic --priority 1 --estimate "1d"
beads.exe create "Task 1.1: AI Metrics Service" --type task --priority 1 --estimate "4h" --tags "week1,observability"
beads.exe create "Task 1.2: Integrate Metrics into AiService" --type task --priority 1 --deps "ved-xxx" --estimate "2h"
beads.exe create "Task 1.3: Establish Baseline Report" --type task --priority 2 --estimate "1h"

beads.exe create "Epic 2: RAG Integration" --type epic --priority 1 --estimate "2d"
beads.exe create "Task 2.1: RAG Adapter Service" --type task --priority 1 --estimate "3h" --tags "week1,rag"
beads.exe create "Task 2.2: Connect RAG to AI Mentor" --type task --priority 1 --deps "ved-xxx" --estimate "4h"
beads.exe create "Task 2.3: Seed Financial Knowledge Base" --type task --priority 2 --estimate "6h" --tags "content"

beads.exe create "Epic 3: Token Optimization" --type epic --priority 1 --estimate "1d"
beads.exe create "Task 3.1: Semantic Response Caching" --type task --priority 1 --estimate "5h" --tags "week1,optimization"
beads.exe create "Task 3.2: Response Length Limits" --type task --priority 2 --estimate "2h"

# Week 2
beads.exe create "Epic 4: Multi-Intent Classification" --type epic --priority 1 --estimate "1d" --deps "ved-xxx (Epic 2)"
beads.exe create "Task 4.1: Extend Intent Classifier" --type task --priority 1 --estimate "4h" --tags "week2"
beads.exe create "Task 4.2: Intent-Specific RAG Filtering" --type task --priority 2 --estimate "3h"

beads.exe create "Epic 5: Behavioral AI Fusion" --type epic --priority 1 --estimate "2d"
beads.exe create "Task 5.1: AI Nudge Variant Generator" --type task --priority 1 --estimate "6h" --tags "week2,behavioral"
beads.exe create "Task 5.2: Proactive AI Trigger Service" --type task --priority 2 --estimate "4h" --tags "hooked-loop"

# Week 3
beads.exe create "Epic 6: Testing + Documentation" --type epic --priority 1 --estimate "2d"
beads.exe create "Task 6.1: Unit Tests (60% Coverage)" --type task --priority 1 --estimate "8h" --tags "week3,testing"
beads.exe create "Task 6.2: Integration Tests" --type task --priority 1 --estimate "6h"
beads.exe create "Task 6.3: Manual Evaluation" --type task --priority 2 --estimate "2h"

beads.exe create "Epic 7: A/B Testing + Launch" --type epic --priority 1 --estimate "2d"
beads.exe create "Task 7.1: A/B Test Framework" --type task --priority 1 --estimate "4h" --tags "week3,launch"
beads.exe create "Task 7.2: Grafana Dashboard" --type task --priority 2 --estimate "4h"
beads.exe create "Task 7.3: Documentation Update" --type task --priority 2 --estimate "3h"
```

---

## ğŸ¯ Success Criteria (Measurable - FIXED)

### Week 1 Checkpoints:
- âœ… `/api/metrics` endpoint returns AI metrics
- âœ… Baseline report shows current performance (3.2s latency, 2,800 tokens/req)
- âœ… RAG adapter retrieves relevant docs in <100ms
- âœ… Semantic cache hit rate >20% (after 50 test queries)

### Week 2 Checkpoints:
- âœ… Intent classifier handles 5 intent types (80%+ accuracy)
- âœ… AI nudge variants generated for 10% of nudges (A/B tracked)
- âœ… Proactive triggers cron runs successfully (check logs)

### Week 3 (Final):
- âœ… **Latency:** P95 <2s (vs 3.2s baseline) = 38% improvement
- âœ… **Token Efficiency:** <1,500 tokens/req (vs 2,800) = 46% reduction
- âœ… **Cache Hit Rate:** >60% (vs 12%) = 5x improvement
- âœ… **RAG Grounding:** 80%+ responses cite sources
- âœ… **A/B Test:** Treatment variant shows +15% satisfaction
- âœ… **Unit Test Coverage:** 60% for new code
- âœ… **Manual Evaluation:** 8/10 average score

---

## ğŸ“Š Risk Management (UPDATED)

### High-Risk Items (Mitigation Strategies)

#### 1. Token Budget Still Exceeded
**Scenario:** Semantic caching only achieves 40% hit rate (not 60%)  
**Impact:** Still 60% of queries hit LLM â†’ budget exhausted in 1 week  
**Mitigation:**
- Implement tiered fallback: If budget <20%, return rule-based responses
- Add daily budget alerts (Prometheus alert rule)
- Increase cache threshold from 0.92 to 0.88 (more aggressive caching)

#### 2. RAG Hallucination (AI Ignores Sources)
**Scenario:** AI generates response but doesn't cite RAG sources  
**Impact:** Users get incorrect financial advice  
**Mitigation:**
- Add post-processing check: If RAG context provided but no citation in response â†’ reject response
- Manual review of 10 responses/week (Task 6.3)
- Add user feedback button ("Was this accurate?")

#### 3. A/B Test Shows No Improvement
**Scenario:** Treatment variant has same or worse satisfaction than control  
**Impact:** Wasted 3 weeks of work  
**Mitigation:**
- Partial rollout: Start with 10% treatment (not 50/50)
- Early exit if statistically significant negative result after 100 users
- Have rollback plan ready (feature flag to disable RAG)

#### 4. Production Load Exceeds Test Scenarios
**Scenario:** 500 concurrent users vs tested 50  
**Impact:** pgvector index locks, Gemini API rate limits  
**Mitigation:**
- Load test with Vegeta before launch: `vegeta attack -rate=100/s -duration=60s`
- Add circuit breaker to Gemini API calls (fail fast if timeout)
- Implement request queuing (BullMQ) for non-urgent queries

---

## ğŸ“ Comparison: Original vs Revised Plan

| Dimension | Original Plan | Revised Plan | Improvement |
|-----------|---------------|--------------|-------------|
| **Architecture** | Trinity System (3 new services) | Smart Extensions (enhance existing) | 80% code reuse |
| **New Code** | 2,000+ lines | 200 lines | 90% reduction |
| **Timeline** | 2 weeks (unrealistic) | 3 weeks (buffered) | 50% more realistic |
| **Token Budget** | Broken (18 queries/day) | Fixed (200+ queries/day) | 11x capacity |
| **Baselines** | None (can't measure) | Established Week 1 | Measurable ROI |
| **Testing** | End-of-sprint only | Continuous (TDD) | Higher quality |
| **Risk Score** | 4/10 (high risk) | 8.5/10 (low risk) | 112% improvement |
| **Behavioral AI** | Rebuild NudgeEngine | Reuse + AI variants | Leverage existing |
| **Complexity** | 5 epics (serial) | 7 epics (parallel) | Faster delivery |

---

## ğŸš€ Next Steps (Immediate Actions)

### Day 1 (Today):
1. **Review this revised plan** with team (30 min standup)
2. **Create Beads tasks** using script above
3. **Start Task 1.1** (AI Metrics Service) - 4 hours

### Week 1 Priorities:
- **Mon-Tue:** Observability (establish baselines)
- **Wed-Thu:** RAG integration (grounding)
- **Fri:** Token optimization (semantic caching)

### Success Celebration Criteria:
- Week 1: First cached response served (cache hit metric >0)
- Week 2: First AI-generated nudge variant sent to user
- Week 3: A/B test shows positive results (+15% satisfaction)

---

**End of Revised Plan** ğŸ¯

---

## ğŸ“ Appendix: Oracle's Key Recommendations (Summary)

1. **Don't rebuild, extend** - Existing `ai.service.ts` already has 80% of needed logic
2. **Fix token math first** - Semantic caching is highest ROI (5x capacity increase)
3. **Measure before changing** - Establish baselines Day 1 (can't improve blindly)
4. **Reuse NudgeEngine** - Don't duplicate behavioral logic, enhance it
5. **Parallel execution** - Run Epics 1-3 concurrently (save 5 days)
6. **Realistic timelines** - 3 weeks (not 2) with buffer for unknowns
7. **A/B test early** - Start with 10% treatment (not 50/50) to limit blast radius

**Overall:** Pragmatic, incremental approach delivers 70% of value with 30% of effort. ğŸš€
